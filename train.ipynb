{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1B36Jz09o9ui1Ht5ThPnO61_0-z_95xaG",
      "authorship_tag": "ABX9TyMSPyMhnIlgN+v/gQ+6XysF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kietbg0079/delete_all_files_google_drive/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHJ_YgzBVSoD"
      },
      "source": [
        "import zipfile as zp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M8zRO_3Wz85"
      },
      "source": [
        "path = '/content/drive/MyDrive/AIP302/flower_data.zip'\n",
        "\n",
        "raw_data = zp.ZipFile(path)\n",
        "raw_data.extractall('/content/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZxowz5TXCyt"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil \n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import models,layers,optimizers\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, BatchNormalization\n",
        "\n",
        "#from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping,  ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.resnet import ResNet101, preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnrtKyzzMYwB"
      },
      "source": [
        "batch_size = 64\n",
        "img_height = 224\n",
        "img_width = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzo61y2RXw5x",
        "outputId": "437b7407-7e68-49cb-a142-f6bc5d927841"
      },
      "source": [
        "TRAINING_DIR = '/content/flower_data/train'\n",
        "\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   horizontal_flip=True,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   )\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(img_height, img_width))\n",
        "\n",
        "\n",
        "VALIDATION_DIR = '/content/flower_data/valid'\n",
        "\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              class_mode='categorical',\n",
        "                                                              target_size=(img_height, img_width))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6552 images belonging to 102 classes.\n",
            "Found 818 images belonging to 102 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOBkpYudYioc"
      },
      "source": [
        "adam = Adam(learning_rate=3e-4)\n",
        "\n",
        "resnet101_base = ResNet101(include_top=True, weights='imagenet',\n",
        "                          input_shape=(img_width, img_height,3))\n",
        "\n",
        "output = resnet101_base.get_layer(index = -1).output  \n",
        "output = Flatten()(output)\n",
        "\n",
        "output = Dense(512,activation = \"relu\")(output)\n",
        "output = BatchNormalization()(output)\n",
        "output = Dropout(0.2)(output)\n",
        "output = Dense(512,activation = \"relu\")(output)\n",
        "output = BatchNormalization()(output)\n",
        "output = Dropout(0.2)(output)\n",
        "output = Dense(102, activation='softmax')(output)\n",
        "\n",
        "resnet101_model = Model(resnet101_base.input, output)\n",
        "for layer in resnet101_model.layers[:-7]:\n",
        "    layer.trainable = False\n",
        "resnet101_model.summary()\n",
        "\n",
        "resnet101_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics =['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkwzso-x80KX"
      },
      "source": [
        "resnet101_model.load_weights('/content/drive/MyDrive/AIL201/AIP302/Landmark_weights_epoch-53_loss-1.1591_val_loss-1.4267.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UdHfYWkYj9-"
      },
      "source": [
        "epoch = 100\n",
        "\n",
        "history = resnet101_model.fit(train_generator,\n",
        "                              epochs=epoch,\n",
        "                              verbose=1,\n",
        "                              validation_data=validation_generator,\n",
        "                              callbacks = [ModelCheckpoint('Landmark_weights_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
        "                                                          monitor='val_loss',\n",
        "                                                          verbose=1,\n",
        "                                                          save_best_only=True,\n",
        "                                                          save_weights_only=True,\n",
        "                                                          mode='auto',\n",
        "                                                          period=1),\n",
        "                                          EarlyStopping(monitor='val_loss',\n",
        "                                                        patience=7, \n",
        "                                                        verbose=1, \n",
        "                                                        min_delta=0.001),\n",
        "                              ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AdPJJn2z8oUR",
        "outputId": "8f969d5c-7839-440f-aa0d-3cf1f5ce9a6d"
      },
      "source": [
        "shutil.move('/content/Landmark_weights_epoch-53_loss-1.1591_val_loss-1.4267.h5', '/content/drive/MyDrive/AIP302')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/AIP302/Landmark_weights_epoch-53_loss-1.1591_val_loss-1.4267.h5'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}